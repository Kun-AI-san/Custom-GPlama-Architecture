{
    "vocab_size": 100277,
    "context_length": 2048,
    "emb_dim": 2048,
    "n_heads": 32,
    "n_layers": 16,
    "drop_rate": 0.0,
    "qkv_bias": false,
    "n_groups": 8,
    "use_flash_attention": true
}